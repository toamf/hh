import torch
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import numpy as np

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Using device:', device)

# загружаем данные
data = pd.read_csv('/kaggle/input/jena-climate-2009-2016/jena_climate_2009_2016.csv')

# определяем размеры обучающей и валидационной выборок
train_size = int(0.8 * len(data))
val_size = len(data) - train_size

# разделяем данные на обучающую и валидационную выборки
train_data = data[:train_size]
val_data = data[train_size:]

# нормализуем данные для обучающей выборки
scaler = MinMaxScaler()
train_data_normalized = scaler.fit_transform(train_data.values[:, 1:])

# нормализуем данные для валидационной выборки на основе статистик обучающей выборки
val_data_normalized = scaler.transform(val_data.values[:, 1:])

# определяем параметры модели
input_size = train_data_normalized.shape[1]
hidden_size = 64
output_size = train_data_normalized.shape[1]
num_layers = 2
sequence_length = 24

# создаем датасет
class JenaClimateDataset(Dataset):
    def __init__(self, data, sequence_length):
        self.data = data
        self.sequence_length = sequence_length

    def __len__(self):
        return len(self.data) - self.sequence_length

    def __getitem__(self, idx):
        x = self.data[idx:idx+self.sequence_length]
        y = self.data[idx+self.sequence_length]
        return torch.Tensor(x), torch.Tensor(y)
        
        
# создаем загрузчики данных
train_dataset = JenaClimateDataset(train_data_normalized, sequence_length)
train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)

val_dataset = JenaClimateDataset(val_data_normalized, sequence_length)
val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)


# создаем класс модели
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_size*2, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.rnn(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

# создаем экземпляр модели
model = RNN(input_size, hidden_size, output_size, num_layers).to(device)

# задаем функцию потерь и оптимизатор
criterion = nn.MSELoss().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# обучаем модель
num_epochs = 100
train_loss = []
val_loss = []
best_val_loss = float('inf')
patience = 5
counter = 0

for epoch in range(num_epochs):
    running_loss = 0.0
    model.train()
    for x, y in train_loader:
        x = x.to(device)
        y = y.to(device)

        optimizer.zero_grad()
        y_pred = model(x)
        loss = criterion(y_pred, y)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * x.size(0)

    epoch_loss = running_loss / len(train_data)
    train_loss.append(epoch_loss)

    val_running_loss = 0.0
    model.eval()
    with torch.no_grad():
        for x, y in val_loader:
            x = x.to(device)
            y = y.to(device)

            y_pred = model(x)
            loss = criterion(y_pred, y)
            val_running_loss += loss.item() * x.size(0)
        val_epoch_loss = val_running_loss / len(val_data)
        val_loss.append(val_epoch_loss)

        print('Epoch [{}/{}], Train Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch+1, num_epochs, train_loss[-1], val_loss[-1]))

        # проверяем условие early stopping
        if val_epoch_loss < best_val_loss:
            best_val_loss = val_epoch_loss
            counter = 0
        else:
            counter += 1
            if counter >= patience:
                print('Early stopping')
                break

torch.save(model.state_dict(), '/kaggle/working/model.pth')

# переводим модель в режим предсказания
model.eval()

# создаем список предсказаний
predictions = []

# начинаем предсказания
with torch.no_grad():
    for x, y in val_loader:
        x = x.to(device)
        y = y.to(device)

        # получаем предсказание модели
        y_pred = model(x)

        # добавляем предсказанные значения в список
        predictions.append(y_pred.cpu().numpy())

# объединяем все предсказания в один массив
predictions = np.concatenate(predictions)

# инвертирование нормализации
y_pred = scaler.inverse_transform(predictions)[:, 1]

# вывод предсказанных значений на следующие 24 часа
print([round(x, 2) for x in y_pred[-144:]])
